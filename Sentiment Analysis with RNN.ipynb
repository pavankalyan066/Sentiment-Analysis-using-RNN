{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries for the implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from Keras to Train, Test variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#load the imdb dataset into train and test set\n",
    "from keras.datasets import imdb\n",
    "vocabulary_size = 5000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print one instance of Review and Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'the', 'clear', 'fact', 'entertaining', 'there', 'life', 'back', 'br', 'is', 'and', 'show', 'of', 'performance', 'stars', 'br', 'actors', 'film', 'him', 'many', 'should', 'movie', 'reasons', 'to', 'and', 'reading', 'and', 'are', 'in', 'of', 'scenes', 'and', 'and', 'of', 'and', 'out', 'compared', 'not', 'boss', 'yes', 'to', 'and', 'show', 'its', 'disappointed', 'fact', 'raw', 'to', 'it', 'justice', 'by', 'br', 'of', 'where', 'clear', 'fact', 'many', 'your', 'way', 'and', 'with', 'city', 'nice', 'are', 'is', 'along', 'wrong', 'not', 'as', 'it', 'way', 'she', 'but', 'this', 'anything', 'up', \"haven't\", 'been', 'by', 'who', 'of', 'choices', 'br', 'of', 'you', 'to', 'as', 'this', \"i'd\", 'it', 'and', 'who', 'of', 'shot', \"you'll\", 'to', 'love', 'for', 'and', 'of', 'you', 'it', 'is', 'sequels', 'of', 'little', 'quest', 'are', 'seen', 'watched', 'front', 'chemistry', 'to', 'simply', 'alive', 'of', 'chris', 'being', 'it', 'is', 'say', 'easy', 'and', 'cry', 'in', 'chemistry', 'but', 'and', 'all', 'it', 'maybe', 'this', 'is', 'wing', 'film', 'job', 'live', 'of', 'and', 'relief', 'and', 'level', 'names', 'and', 'and', 'to', 'be', 'stops', 'serial', 'and', 'watch', 'is', 'men', 'go', 'this', 'of', 'wing', 'american', 'from', 'and', 'moving', 'is', 'accepted', 'put', 'this', 'of', 'jerry', 'for', 'places', 'so', 'work', 'and', 'watch', 'and', 'lot', 'br', 'that', 'from', 'sometimes', 'wondered', 'make', 'department', 'introduced', 'to', 'wondered', 'from', 'action', 'at', 'turns', 'in', 'low', 'that', 'in', 'gay', \"i'm\", 'of', 'chemistry', 'bible', 'i', 'i', 'simply', 'alive', 'it', 'is', 'time', 'done', 'inspector', 'to', 'watching', 'look', 'world', 'named', 'for', 'more', 'tells', 'up', 'many', 'fans', 'are', 'that', 'movie', 'music', 'her', 'get', 'grasp', 'but', 'seems', 'in', 'people', 'film', 'that', 'if', 'explain', 'in', 'why', 'for', 'and', 'find', 'of', 'where', 'br', 'if', 'and', 'movie', 'throughout', 'if', 'and', 'of', 'you', 'best', 'look', 'red', 'and', 'to', 'recently', 'in', 'successfully', 'much', 'unfortunately', 'going', 'dan', 'and', 'stuck', 'is', 'him', 'sequences', 'but', 'of', 'you', 'of', 'enough', 'for', 'its', 'br', 'that', 'beautiful', 'put', 'reasons', 'of', 'chris', 'chemistry', 'wing', 'and', 'for', 'of', 'you', 'red', 'time', 'and', 'to', 'as', 'companion', 'and', 'of', 'chris', 'less', 'br', 'of', 'subplots', 'torture', 'in', 'low', 'alive', 'in', 'gay', 'some', 'br', 'of', 'wing', 'if', 'time', 'actual', 'in', 'also', 'side', 'any', 'if', 'name', 'takes', 'for', 'of', 'friendship', 'it', 'of', '10', 'for', 'had', 'and', 'great', 'to', 'as', 'you', 'students', 'for', 'movie', 'of', 'going', 'and', 'for', 'bad', 'well', 'best', 'had', 'at', 'woman', 'br', 'musical', 'when', 'it', 'caused', 'of', 'gripping', 'to', 'as', 'gem', 'in', 'and', 'for', 'and', 'look', 'end', 'gene', 'in', 'at', 'world', 'aliens', 'of', 'you', 'it', 'meet', 'but', 'is', 'quite', 'br', 'western', 'ideas', 'of', 'chris', 'little', 'of', 'films', 'he', 'an', 'time', 'done', 'this', 'were', 'right', 'too', 'to', 'of', 'enough', 'for', 'of', 'ending', 'become', 'family', 'beautiful', 'are', 'make', 'right', 'being', 'it', 'time', 'much', 'bit', 'especially', 'craig', 'for', 'of', 'you', 'parts', 'bond', 'who', 'of', 'here', 'parts', 'at', 'due', 'given', 'movie', 'of', 'once', 'give', 'find', 'actor', 'to', 'recently', 'in', 'at', 'world', 'dolls', 'loved', 'and', 'it', 'is', 'video', 'him', 'fact', 'you', 'to', 'by', 'br', 'of', 'where', 'br', 'of', 'grown', 'fight', 'culture', 'leads']\n",
      "sentiment\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#understanding how the dataset looks like\n",
    "words = imdb.get_word_index()\n",
    "\n",
    "#the words are already vectorized in the dataset, hence we reverse the process to see the word distribution\n",
    "vects = {i: word for word, i in words.items()}\n",
    "print('review')\n",
    "print([vects.get(i, ' ') for i in X_train[10]])\n",
    "\n",
    "#the sentiment is 1 if the review is positive and 0 if the review is negative\n",
    "print('sentiment')\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the data sequence to make the inputs to same length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the RNN to work all our input dependencies must have same length \n",
    "total_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=total_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Basic LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#we will build a simple LSTM model with one embedding layer, one LSTM and one output layer\n",
    "embedding_size=32\n",
    "max_words = 500\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model by passing the optimizer and loss function and the evaluation metric\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the parameters and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "195/195 [==============================] - 253s 1s/step - loss: 0.4719 - accuracy: 0.7695 - val_loss: 0.2982 - val_accuracy: 0.8750\n",
      "Epoch 2/5\n",
      "195/195 [==============================] - 270s 1s/step - loss: 0.3718 - accuracy: 0.8473 - val_loss: 0.3391 - val_accuracy: 0.8438\n",
      "Epoch 3/5\n",
      "195/195 [==============================] - 279s 1s/step - loss: 0.2662 - accuracy: 0.8949 - val_loss: 0.2683 - val_accuracy: 0.8906\n",
      "Epoch 4/5\n",
      "195/195 [==============================] - 291s 1s/step - loss: 0.2552 - accuracy: 0.9012 - val_loss: 0.3403 - val_accuracy: 0.8594\n",
      "Epoch 5/5\n",
      "195/195 [==============================] - 286s 1s/step - loss: 0.2145 - accuracy: 0.9187 - val_loss: 0.2936 - val_accuracy: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de1e66a548>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data to the model and begin training\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "x_val, y_val = X_train[:batch_size], y_train[:batch_size]\n",
    "xtrain, ytrain = X_train[batch_size:], y_train[batch_size:]\n",
    "model.fit(xtrain, ytrain, validation_data=(x_val, y_val), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8718000054359436\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model accuracy on unseen test data\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-96a81c76aac2>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "Prediction:  [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "#make model predictions on test data\n",
    "print(\"Prediction: \",model.predict_classes(X_test[1:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:  [1 1 0 1 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#compare the model prediction with actual data\n",
    "print(\"Actual: \",y_test[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
